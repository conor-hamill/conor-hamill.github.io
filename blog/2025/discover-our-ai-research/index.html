<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xH3ACvn2Y4WivW82n26QJQ.png"></figure> <h3>2025 publications</h3> <h4><strong>Evaluating the Sensitivity of LLMs to Prior Context</strong></h4> <p>As large language models (LLMs) are increasingly deployed in multi-turn dialogue and other sustained interactive scenarios, it is essential to understand how extended context affects their performance. Popular benchmarks, focusing primarily on single-turn question answering (QA) tasks, fail to capture the effects of multi-turn exchanges. To address this gap, we introduce a novel set of benchmarks that systematically vary the volume and nature of prior context. We evaluate multiple conventional LLMs, including GPT, Claude, and Gemini, across these benchmarks to measure their sensitivity to contextual variations. Our findings reveal that LLM performance on multiple-choice questions can degrade dramatically in multi-turn interactions, with performance drops as large as 73% for certain models. Even highly capable models such as GPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative performance of larger versus smaller models is not always predictable. Moreover, the strategic placement of the task description within the context can substantially mitigate performance drops, improving the accuracy by as much as a factor of 3.5. These findings underscore the need for robust strategies to design, evaluate, and mitigate context-related sensitivity in LLMs.</p> <p><a href="https://arxiv.org/abs/2506.00069" rel="external nofollow noopener" target="_blank">[2506.00069] Evaluating the Sensitivity of LLMs to Prior Context</a></p> <h4><strong>AUTOSUMM: A Comprehensive Framework for LLM-Based Conversation Summarization</strong></h4> <p>We present AUTOSUMM, a large language model (LLM)-based summarization system to generate accurate, privacy-compliant summaries of customer-advisor conversations. The system addresses challenges unique to this domain, including speaker attribution errors, hallucination risks, and short or low-information transcripts. Our architecture integrates dynamic transcript segmentation, thematic coverage tracking, and a domain specific multi-layered hallucination detection module that combines syntactic, semantic, and entailment-based checks.</p> <p><em>Accepted to the Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Industry Track)</em></p> <h4><strong>How Personality Traits Shape LLM Risk-Taking Behaviour</strong></h4> <p>This research investigates the relationship between personality traits and risk-taking behaviour in Large Language Models (LLMs) using Cumulative Prospect Theory and the Big Five personality framework. The study reveals that most examined LLMs function as risk-neutral rational agents while exhibiting higher Conscientiousness and Agreeableness with lower Neuroticism. Interventions targeting Big Five traits, especially Openness, successfully influence risk-propensity in several models. Advanced LLMs demonstrate human-like personality-risk patterns through optimal prompting, while their distilled variants show limitations in cognitive bias transfer. The research identifies Openness as the most significant factor affecting risk-propensity, consistent with human baselines, and highlights both potential and limitations of personality-based interventions in LLM decision-making.</p> <p><em>Accepted to the Findings of the Association for Computational Linguistics: ACL 2025</em></p> <h4>Application of GraphSAGE in Complex Transaction Networks</h4> <p>We present the practical application of GraphSAGE, an inductive Graph Neural Network framework, to non-bipartite heterogeneous transaction networks within a banking context. We construct a transaction network from anonymised customer and merchant transactions and train a GraphSAGE model to generate node embeddings. Our exploratory work on the embeddings reveals interpretable clusters aligned with geographic and demographic attributes. Additionally, we illustrate their utility in downstream classification tasks by applying them to a money mule detection model where using these embeddings improve the prioritisation of high-risk accounts. Beyond fraud detection, our work highlights GraphSAGE’s adaptability to banking-scale networks, emphasising its inductive capability, scalability, and interpretability. This study provides a blueprint for financial institutions to harness graph machine learning for actionable insights in transactional ecosystems.</p> <p><em>Accepted to the Proceedings of GbR 2025</em></p> <h4>Agent-based Modelling of Credit Card Promotions</h4> <p>In this research work, we develop an agent-based model of the UK credit card market, based on the interactions between lenders and customers. We then show how this model can be used as a tool to explore outcomes of zero-interest credit card promotion strategies under different market scenarios.</p> <p><a href="https://arxiv.org/abs/2311.01901" rel="external nofollow noopener" target="_blank">[2311.01901] Agent-based Modelling of Credit Card Promotions</a></p> <p><a href="https://www.emerald.com/insight/publication/issn/0265-2323" rel="external nofollow noopener" target="_blank">International Journal of Bank Marketing</a>: <a href="https://www.emerald.com/insight/content/doi/10.1108/ijbm-02-2024-0082/full/html" rel="external nofollow noopener" target="_blank">Agent-based modelling of credit card promotions</a></p> <h3>2024 publications</h3> <h4>A Brief Review of Quantum Machine Learning for Financial Services</h4> <p>This review paper examines state-of-the-art algorithms and techniques in quantum machine learning with potential applications in finance. We discuss QML techniques in supervised learning tasks, such as Quantum Variational Classifiers, Quantum Kernel Estimation, and Quantum Neural Networks (QNNs), along with quantum generative AI techniques like Quantum Transformers and Quantum Graph Neural Networks (QGNNs). The financial applications considered include risk management, credit scoring, fraud detection, and stock price prediction. We also provide an overview of the challenges, potential, and limitations of QML, both in these specific areas and more broadly across the field. We hope that this can serve as a quick guide for data scientists, professionals in the financial sector, and enthusiasts in this area to understand why quantum computing and QML in particular could be interesting to explore in their field of expertise.</p> <p><a href="https://arxiv.org/abs/2407.12618v1" rel="external nofollow noopener" target="_blank">[2407.12618v1] A Brief Review of Quantum Machine Learning for Financial Services</a></p> <h3>2023 publications</h3> <h4>Conformal Predictions for Longitudinal Data</h4> <p>In this paper, we present our research into uncertainty calculation for multi-dimensional time-series forecasting, setting a new performance benchmark for the field. These uncertainty estimates allow more informative predictions in areas like demand and stock market prediction and boost the trustworthiness of our forecasts.</p> <p><a href="https://arxiv.org/abs/2310.02863" rel="external nofollow noopener" target="_blank">[2310.02863] Conformal Predictions for Longitudinal Data</a></p> <h4>Modelling customer lifetime-value in the retail banking industry</h4> <p>This research improves the accuracy of customer lifetime value estimation in retail banking through a machine-learning framework. The improved insight from this framework allows accurate identification of high-value customers, informing marketing, relationship management, and business growth strategies.</p> <p><a href="https://arxiv.org/abs/2304.03038" rel="external nofollow noopener" target="_blank">[2304.03038] Modelling customer lifetime-value in the retail banking industry</a></p> <h3>2022 publications</h3> <h4>Offline Deep Reinforcement Learning for Dynamic Pricing of Consumer Credit</h4> <p>In this paper, we train an offline reinforcement learning agent with a static dataset to determine a better loan interest pricing policy. This approach can be applied to other pricing tasks and means risky experimentation in a live environment can be avoided.</p> <p><a href="https://dl.acm.org/doi/proceedings/10.1145/3533271" rel="external nofollow noopener" target="_blank">ICAIF ’22: Proceedings of the Third ACM International Conference on AI in Finance</a></p> <p><a href="https://arxiv.org/abs/2203.03003" rel="external nofollow noopener" target="_blank">[2203.03003] Offline Deep Reinforcement Learning for Dynamic Pricing of Consumer Credit</a></p> <h4>An Introduction to Machine Unlearning</h4> <p>This paper presents a comprehensive review of a wide range of machine unlearning algorithms, which remove the influence of individual observations from models while minimising computational costs. This review standardizes definitions, evaluation methods, and tackles implementation challenges for machine unlearning.</p> <p><a href="https://arxiv.org/abs/2209.00939" rel="external nofollow noopener" target="_blank">[2209.00939] An Introduction to Machine Unlearning</a></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fdcdd666c5af" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://nwg.ai/discover-our-ai-research-fdcdd666c5af" rel="external nofollow noopener" target="_blank">Discover our AI research</a> was originally published in <a href="https://nwg.ai" rel="external nofollow noopener" target="_blank">NatWest Group AI &amp; Engineering</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>