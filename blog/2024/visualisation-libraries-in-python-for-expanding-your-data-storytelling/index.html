<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>5 python packages for expanding your data storytelling</h3> <p>Data visualisation is a core part of how a data scientist tells a story, encompassing how they explore their data, share insights, and explain the impact of their models in their specific domain. The three most popular python libraries for visualisation are <a href="https://matplotlib.org/" rel="external nofollow noopener" target="_blank">matplotlib</a>, <a href="https://seaborn.pydata.org/" rel="external nofollow noopener" target="_blank">seaborn</a>, and <a href="https://plotly.com/" rel="external nofollow noopener" target="_blank">plotly</a>, with many other frameworks allowing the creation of impactful plots, like <a href="https://altair-viz.github.io/" rel="external nofollow noopener" target="_blank">Altair</a>, <a href="http://bokeh.org/" rel="external nofollow noopener" target="_blank">Bokeh</a>, and <a href="https://yhat.github.io/ggpy/" rel="external nofollow noopener" target="_blank">ggplot</a>.</p> <p>However, the core bread-and-butter python libraries that make up the skillsets of (many) modern data scientists are based around pandas, for data manipulation and analysis; scikit-learn, for pre-processing and modelling; and matplotlib for visualisation, respectively. Being able to expand upon these frameworks without having to learn a new framework and syntax allows a lot of impact with little effort and re-skilling.</p> <p>Inspired by this thought, this article explores five visualisation libraries that focus on expanding the capabilities of these core packages, including some thoughts on how well they deliver on their goals and which ones are worth adding to your arsenal. The five data visualisation libraries covered will be <a href="https://www.scikit-yb.org/en/latest/index.html" rel="external nofollow noopener" target="_blank">Yellowbrick</a>, <a href="https://rasbt.github.io/mlxtend/" rel="external nofollow noopener" target="_blank">Mlxtend</a>, <a href="https://github.com/matplotlib/mplfinance" rel="external nofollow noopener" target="_blank">mplfinance</a>, <a href="https://mpld3.github" rel="external nofollow noopener" target="_blank">mpld3</a>, and <a href="https://posit-dev.github.io/great-tables/articles/intro.html" rel="external nofollow noopener" target="_blank">great_tables</a>.</p> <h3>Yellowbrick</h3> <ul><li>Docs: <a href="https://www.scikit-yb.org/en/latest/index.html" rel="external nofollow noopener" target="_blank">https://www.scikit-yb.org/en/latest/index.html</a> </li></ul> <p>The summary on its website gives a concise description of the package: <strong>“Yellowbrick extends the Scikit-Learn API to make model selection and hyperparameter tuning easier. Under the hood, it’s using Matplotlib”</strong>. It includes visualisations for many aspects of machine learning, including feature analysis, regression, classification, and clustering. Each of the visualisers on offer is implemented as a function and as a class, which simply wraps the function.</p> <p>To experiment with how flexible this would be with matplotlib subplots, I took the California Housing toy dataset from scikit-learn, for regression problems, and made a single figure with plots that show both the fit of the model and the distribution of the residuals of the fit. The visualisers can take scikit-learn pipelines as inputs, as well as classifiers, making reuse of scikit-learn code convenient.</p> <p>While I wasn’t able to get plotting on multiple matplotlib axes objects using the class-based API for visualisers, the “quick draw” functions shown below worked well, producing a plot that showed my regression fit, while also quickly showing that my residuals weren’t ideally distributed for a linear regression model. This implementation is shown in the code block below.</p> <pre>from sklearn.datasets import fetch_california_housing<br>import pandas as pd<br><br>from sklearn.pipeline import make_pipeline<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>from sklearn.linear_model import LinearRegression<br>from sklearn.metrics import r2_score<br>import matplotlib.pyplot as plt<br><br>from yellowbrick.regressor.prediction_error import prediction_error<br>from yellowbrick.regressor import residuals_plot<br><br># Getting example dataset<br>X, y = fetch_california_housing(return_X_y=True, as_frame=True)<br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)<br><br>feature_cols = [<br>    "MedInc",<br>    "HouseAge",<br>    "AveRooms",<br>    "AveBedrms",<br>    "Population",<br>    "AveOccup",<br>]<br><br>pipeline = make_pipeline(<br>    StandardScaler(),<br>    LinearRegression(),<br>)<br><br>pipeline.fit(X_train[feature_cols], y_train)<br><br>fig, axs = plt.subplots(2, 1, figsize=(14, 12));<br><br># quick draw method<br>visualiser_errors = prediction_error(<br>    pipeline,<br>    X_train[feature_cols],<br>    y_train,<br>    X_test[feature_cols],<br>    y_test,<br>    show=False,<br>    shared_limits=False,<br>    ax=axs[0],<br>)<br><br># quick draw method<br>visualiser_residuals = residuals_plot(<br>    pipeline,<br>    X_train[feature_cols],<br>    y_train,<br>    X_test[feature_cols],<br>    y_test,<br>    hist=False,<br>    qqplot=True,<br>    show=False,<br>    ax=axs[1],<br>)<br><br>visualiser_errors.ax.set_xlim([0, 6])<br>visualiser_errors.ax.set_ylim([0, 8])<br>visualiser_errors.ax.legend(loc=2)<br><br>fig</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bgjf5RZJTe83bmrycYNpQA.png"><figcaption>Plot fit and residuals for linear regression model on the California Housing dataset, visualised using Yellowbrick.</figcaption></figure> <p>The plot above gives the insight I was looking for and only required two function calls to plot (plus some formatting).</p> <p><strong>My take: </strong>this package contains lots of functionality for informative visualisation, while allowing the reuse of scikit-learn pipelines and formatting using object-orientated matplotlib. I’d recommend this to any data scientist looking for a quick and easy way to visualise their models.</p> <h3>Mlxtend</h3> <ul><li>Docs: <a href="https://rasbt.github.io/mlxtend/" rel="external nofollow noopener" target="_blank">https://rasbt.github.io/mlxtend/</a> </li></ul> <p>This package from the well-known data science researcher, author, and developer of <a href="https://lightning.ai/" rel="external nofollow noopener" target="_blank">PyTorch Lightning</a>, <a href="https://sebastianraschka.com/" rel="external nofollow noopener" target="_blank">Sebastian Raschka</a>, introduces itself as “<strong>Mlxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks”. </strong>While not exclusively focussed on visualisation, it contains some visualisation options, with matplotlib as the backend.</p> <p>To give this package a spin, I decided to use the plot_decision_region() function, to compare the decision regions of the logistic regression, SVC, and adaboost classifiers on the penguins dataset. The code below shows the implementation of the plotting of three sub-plots, with an accuracy for each classifier as the title of the subplots.</p> <pre>from sklearn.linear_model import LogisticRegression<br>from sklearn.svm import SVC<br>from sklearn.ensemble import HistGradientBoostingClassifier, AdaBoostClassifier<br>from sklearn.impute import SimpleImputer<br>from sklearn.preprocessing import OneHotEncoder, LabelEncoder<br>from sklearn.compose import ColumnTransformer<br>from sklearn.metrics import accuracy_score<br>import numpy as np<br>from mlxtend.plotting import plot_decision_regions<br><br># penguins dataset downloaded locally<br>penguin_filepath = "../data/penguins_size.csv"<br>df_penguins = pd.read_csv(penguin_filepath)<br><br>penguin_feature_cols = [<br>    "culmen_length_mm",<br>    "culmen_depth_mm",<br>    # 'flipper_length_mm', 'body_mass_g',<br>]<br><br>le = LabelEncoder()<br><br>X = df_penguins[penguin_feature_cols]<br>y = le.fit_transform(df_penguins["species"])<br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)<br><br>simple_inputer = SimpleImputer()<br>X_train = simple_inputer.fit_transform(X_train)<br>X_test = simple_inputer.transform(X_test)<br><br>lr_model = LogisticRegression().fit(X_train, y_train)<br>svc_model = SVC().fit(X_train, y_train)<br>adab_model = AdaBoostClassifier().fit(X_train, y_train)<br><br># plotting<br>figure, ax = plt.subplots(1, 3, figsize=(16, 6))<br><br>clf_names = ["Logistic regression", "SVC", "AdaBoost"]<br>clfs = [lr_model, svc_model, adab_model]<br><br>xlabel, ylabel = penguin_feature_cols<br>for clf, clf_name, ax in zip(clfs, clf_names, axes):<br>    plot_decision_regions(X_test, y_test, clf=clf, ax=ax)<br><br>    acc = accuracy_score(y_test, clf.predict(X_test))<br><br>    ax.set_xlabel(xlabel)<br>    ax.set_ylabel(ylabel)<br>    ax.set_title(f"{clf_name}: accuracy: {100.0 * acc:.2f}%")<br><br>figure</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*G3oAAciRBEtMQhgwADBEZA.png"><figcaption>Decision regions for logistic regression, SVC, and adaboost classifiers on penguins dataset, visualised using the Mlxtend package.</figcaption></figure> <p>Relatively easily, we are able to plot the decision regions for each of these classifiers on the axes objects. These plots show how each of these classifiers separate predictions based on features, including exhibiting the overfitting of the adaboost classifier. We did have to convert the data to numpy arrays, as this function doesn’t accept pandas dataframes as input, despite pandas now accepting dataframes as input for transformers. Given how busy Sebastian Raschka has been with his other projects, I don’t think we can be too harsh about this.</p> <p><strong>My take: </strong>This package has plotting functionality that allows the user to get further insights into their models by re-using their scikit-learn models and enabling the use of matplotlib to configure the plots as you wish. The other functionality offered by the package is also definitely worth exploring.</p> <h3>mplfinance</h3> <ul><li>Docs: <a href="https://github.com/matplotlib/mplfinance" rel="external nofollow noopener" target="_blank">https://github.com/matplotlib/mplfinance</a> </li></ul> <p>This package describes itself as <strong>“matplotlib utilities for the visualization, and visual analysis, of financial data”</strong>. The README in the GitHub repo describes the history of the package, which began as code extracted from the depreciated matplotlib.finance package, and had a previous form under the name mpl-finance. The package focuses on the visualisation of financial data (e.g. stock prices) over time and provides several examples of how to do this in its <a href="https://github.com/matplotlib/mplfinance/tree/master/examples" rel="external nofollow noopener" target="_blank">examples directory</a>.</p> <p>To give this package a try, I downloaded a sample of Apple’s stock price and plotted a candlestick plot for a sample of the stock price back in 2004.</p> <pre>import mplfinance as mpf<br>import matplotlib.pyplot as plt<br><br>df_AAPL = pd.read_csv("../data/AAPL.csv.zip")<br><br>df_AAPL.set_index("Date", inplace=True)<br>df_AAPL.index = pd.to_datetime(df_AAPL.index)<br>df_AAPL = df_AAPL[(df_AAPL.index &gt; "2004-02-01") &amp; (df_AAPL.index &lt; "2004-05-01")]<br><br>mpf.plot(df_AAPL, type="candle", style="binance")</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*93uq1AAk-EZSm3_fL4ZVcA.png"><figcaption>Apple stock in early 2004, plotted using mplfinance.</figcaption></figure> <p>This shows a candlestick plot that looks decent, with one line of plotting. Provided the dataframe passed has the correct headings and the index set as the date, mplfinance can pick up the details it needs to create the plot.</p> <p>The package also has the option to add more information on the daily stock prices to the plots. Using the same data, I used the addplot option of mpl.plot to display the high and low values for each day in one subplot, above a subplot that showed the volume of stock traded every day and the range every day.</p> <pre>df_AAPL["range"] = df_AAPL["High"] - df_AAPL["Low"]<br><br>add_plots = [<br>    mpf.make_addplot(df_AAPL[["High", "Low"]]),<br>    mpf.make_addplot(df_AAPL["range"], panel=1, color="g"),<br>]<br>mpf.plot(df_AAPL, addplot=add_plots, volume=True)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-MMwHE-NnJr7MRm-ODWe4w.png"><figcaption>High and low values for Apple’s stock price in early 2004, above the volume traded and range for the stock, plotted using mpl.finance.</figcaption></figure> <p>This plot contains a lot of information, created with relatively few lines of code. However, the right-hand side y-axis on the lower plot is missing a label. Quite often, plotting functions like this will return the figure or axes objects automatically. However, in the doc string for the plot() function, there is no description of how to access this, or what the function can return. I dug into the source code and found that some of the kwargs provided to the function do have effects, including returnfig=True returning the figure and axes objects. From this, I was able to access the axis object I needed and add a label, but having to dig to find this functionality was fairly inconvenient.</p> <pre>fig, *axes_objects = mpf.plot(df_AAPL, addplot=add_plots, volume=True, returnfig=True)<br>axes_objects[0][3].set_ylabel("Range")<br><br>fig</pre> <p><strong>My take: </strong>This package does enable visualisation of financial data in few lines of code. However, it is let down by the user having to really dig in to find the full functionality, and the slightly strange make_addplot API. That being said, this package has a lot of potential and the number of examples and plotting styles are good. A possible alternative for users would be the <a href="https://plotly.com/python/candlestick-charts/" rel="external nofollow noopener" target="_blank">implementation of candlestick charts in plotly</a>, which has the strong advantage of being able to zoom in on specific ranges in a long time series.</p> <h3>mpld3</h3> <ul><li>Docs: <a href="https://mpld3.github" rel="external nofollow noopener" target="_blank">https://mpld3.github</a> </li></ul> <p>This package <strong>“brings together </strong><a href="http://www.matplotlib.org/" rel="external nofollow noopener" target="_blank"><strong>Matplotlib</strong></a><strong>…and </strong><a href="http://d3js.org/" rel="external nofollow noopener" target="_blank"><strong>D3js</strong></a><strong>, the popular JavaScript library for creating interactive data visualizations for the web”</strong>, setting out to tackle one of matplotlib’s biggest weaknesses - its lack of interactivity. The docs contain some nice examples of how this can be achieved in matplotlib plots, along with exporting the resulting plots as html.</p> <p>I used the penguins dataset again to give this library a try. One of the common problems when visualising large datasets is having overlapping distributions of several groups, making it difficult to observe and compare individual distributions. After trying to replicate the interactive legend example for histograms for each group, I had no success. After some searching, I concluded the containers that are used for histograms aren’t supported by mpld3, but I’d be delighted to be shown I’m wrong. Fortunately, I was able to get the scatter plot functionality working for the penguins dataset, giving me some information about the distribution of the two features, which would be very useful on a larger dataset.</p> <pre>fig, ax = plt.subplots(figsize=(10, 6))<br><br>ax.grid(True, alpha=0.3)<br><br>for species in df_penguins["species"].unique():<br>    (l,) = ax.plot(<br>        df_penguins[df_penguins["species"] == species]["culmen_depth_mm"].values,<br>        df_penguins[df_penguins["species"] == species]["culmen_length_mm"].values,<br>        label=species,<br>        marker=".",<br>        markersize=20,<br>        linestyle="None",<br>    )<br><br>handles, labels = ax.get_legend_handles_labels()  # return lines and labels<br>interactive_legend = plugins.InteractiveLegendPlugin(<br>    ax.lines, labels, alpha_unsel=0.1, alpha_over=1.5, start_visible=True<br>)<br><br>plugins.connect(fig, interactive_legend)<br><br>ax.set_xlabel("Culmen depth [mm]")<br>ax.set_ylabel("Culmen length [mm]")<br>ax.set_title("Culmen length vs depth", size=20)<br><br>mpld3.display(fig)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*I6-7z_AaeW1R5y2qaNxuag.png"></figure> <p>The above picture obviously doesn’t display the functionality, but by clicking on the legend in the notebook the individual groups will disappear, and this can also be exported to a html file. Also, before the plugins from mpld3 get used, no changes to the code are needed, meaning existing matplotlib code can be easily extended to give this functionality.</p> <p><strong>My take: </strong>This package makes a solid attempt towards adding interactivity to matplotlib, and has an appealing syntax in that existing matplotlib scripts can have a few lines of code appended to give interactivity. However, it doesn’t work for all types of matplotlib plots, and <a href="https://mpld3.github.io/faq.html#general" rel="external nofollow noopener" target="_blank">judging by the FAQ</a>, adding interactivity simply won’t be possible for some types of plots, meaning following a plotly example might be your best bet if you’re starting from scratch and want interactivity.</p> <h3>great_tables</h3> <ul><li>Docs: <a href="https://posit-dev.github.io/great-tables/articles/intro.html" rel="external nofollow noopener" target="_blank">https://posit-dev.github.io/great-tables/articles/intro.html</a> </li></ul> <p>While displaying data in tabular format might not be the first thing people think of when they hear data visualisation, a well-structure table of aggregated data in a table can be an effective way of displaying information for a report or presentation, and being able to generate these tables automatically makes life much easier. This package introduces itself as <strong>“a Python package for creating great-looking display tables”</strong>, and makes the distinction that this is aimed to offer features to create tables “you’d find in a web page, a journal article, or magazine”.</p> <p>To give this package a go and see how pretty a table I could create, I downloaded a table of New York property prices from Kaggle and set about displaying some aggregate properties for each area, included in the code snippet below.</p> <pre>from great_tables import GT<br>from great_tables import html, md<br><br>import calendar<br><br>df_NY = pd.read_csv(NY_housing_path)<br><br>NO_BEDS_COL = "No. beds"<br>NO_BATHS_COL = "No. baths"<br>FLOOR_AREA_COL = "Floor area [sqft]"<br>NO_LISTINGS_COL = "No. listings"<br>PRICE_COL = "Price"<br>LOCALITY_COL = "NY Locality"<br><br>rename_dict = {<br>    "LOCALITY": LOCALITY_COL,<br>    "BEDS": NO_BEDS_COL,<br>    "BATH": NO_BATHS_COL,<br>    "PROPERTYSQFT": FLOOR_AREA_COL,<br>    "ADDRESS": NO_LISTINGS_COL,<br>    "PRICE": PRICE_COL,<br>}<br><br>GT( # pandas dataframe serves as inupt to GT onject<br>    pd.concat( # aggregating data using pandas<br>        [<br>            df_NY.groupby(by=["LOCALITY"])[<br>                ["BEDS", "BATH", "PROPERTYSQFT", "PRICE"]<br>            ].median(),<br>            df_NY.groupby(by=["LOCALITY"])[["ADDRESS"]].count(),<br>        ],<br>        axis=1,<br>    )<br>    .reset_index()<br>    .rename(columns=rename_dict)<br>).fmt_integer( # table display from chained methods<br>    columns=[NO_BEDS_COL, NO_BATHS_COL, FLOOR_AREA_COL, NO_LISTINGS_COL]<br>).fmt_currency(<br>    columns=PRICE_COL<br>).tab_spanner(<br>    label=md("*Median property properties*"),<br>    columns=[NO_BEDS_COL, NO_BATHS_COL, FLOOR_AREA_COL, PRICE_COL],<br>).tab_source_note(<br>    source_note=f"Source: {source_address}"<br>).tab_header(<br>    title=html("&lt;strong&gt;Properties across New York&lt;/strong&gt;"),<br>    subtitle=html("Average property properties by New York localities"),<br>)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vAMobWeqWbdzh2IspHuaKQ.png"></figure> <p>As the code snippet shows, the ability to chain methods together allows fine control of the components of the table, including formatting and naming the source.</p> <p>A similar exercise below on sales from a Walmart dataset allowed the sales across different stores to be summarised easily.</p> <pre>df_sales = pd.read_csv(WALMART_SALES_FILE)<br>df_sales["Date"] = pd.to_datetime(df_sales["Date"], format="%d-%m-%Y")<br><br># Only selecting some stores as a sample<br>df_sales = df_sales.loc[df_sales["Store"] &lt; 6, :]<br><br>df_sales.loc[:, "Month"] = df_sales["Date"].dt.month<br><br># calculating percentage of sales per month<br>groupby = df_sales.groupby(by=["Month", "Store"])["Weekly_Sales"].mean().unstack()<br>groupby_percentage_sales_per_month = groupby / groupby.sum(axis=0)<br>groupby_percentage_sales_per_month.reset_index(inplace=True)<br><br>groupby_percentage_sales_per_month["Month"] = groupby_percentage_sales_per_month[<br>    "Month"<br>].apply(lambda x: calendar.month_abbr[x])<br><br>groupby_percentage_sales_per_month.columns = (<br>    groupby_percentage_sales_per_month.columns.astype("str")<br>)<br><br>GT(<br>    groupby_percentage_sales_per_month,<br>    rowname_col="Month",<br>).data_color(<br>    # domain=[0.8, 0.10],<br>    palette=["rebeccapurple", "white", "orange"],<br>    na_color="white",<br>).tab_header(<br>    title="Average percentage yearly sales by month for five Walmart stores",<br>    # subtitle=html("Average monthly values at latitude of 20&amp;deg;N."),<br>).tab_source_note(<br>    source_note=f"Source: https://www.kaggle.com/datasets/mikhail1681/walmart-sales"<br>).fmt_percent(<br>    [<br>        "1",<br>        "2",<br>        "3",<br>        "4",<br>        "5",<br>    ],<br>    decimals=2,<br>)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yRQ9M4wcVhIdKQIFqCCPNw.png"></figure> <p>Again, with a few chained methods, we have a table that allows us to clearly see some trends across these five different shops. You can quickly see how (unsurprisingly) sales peak in December, with lower fractions of sales in January and July.</p> <p><strong>My take: </strong>This is a well-developed package that makes creating tables for presentation straightforward. Taking pandas dataframes as inputs to the GT class, along with the option of chaining of formatting methods, makes this a straightforward package to adapt. As a researcher, if this could export to LaTeX format, that would be amazing. Thankfully <a href="https://github.com/posit-dev/great-tables/issues/75" rel="external nofollow noopener" target="_blank">someone has already submitted an issue asking for this as a feature</a>, with an estimation of “done in early-to-mid 2024”, so this feature is eagerly awaited</p> <h3>Conclusion</h3> <p>Hopefully you found this article somewhat interesting and useful. I believe the breadth of the packages here strongly underline the strengths of the scientific python stack. The notebook the plots and tables in this article were created in can be found on my GitHub <a href="https://github.com/conorhamill36/viz_packages" rel="external nofollow noopener" target="_blank">here</a>. Please share any constructive criticism, including anything key in the packages I missed, and any other visualisation packages that expand upon pandas/scikit-learn/matplotlib well!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e0dc4ca54302" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/swlh/visualisation-libraries-in-python-for-expanding-your-data-storytelling-e0dc4ca54302" rel="external nofollow noopener" target="_blank">Visualisation libraries in python for expanding your data storytelling</a> was originally published in <a href="https://medium.com/swlh" rel="external nofollow noopener" target="_blank">The Startup</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>