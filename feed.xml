<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://conor-hamill.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://conor-hamill.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-19T19:05:39+00:00</updated><id>https://conor-hamill.github.io/feed.xml</id><title type="html">Conor B. Hamill</title><subtitle>Personal website for Conor Hamill. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">My 2024 in reading</title><link href="https://conor-hamill.github.io/blog/2024/my-2024-in-reading/" rel="alternate" type="text/html" title="My 2024 in reading"/><published>2024-12-29T12:30:00+00:00</published><updated>2024-12-29T12:30:00+00:00</updated><id>https://conor-hamill.github.io/blog/2024/my-2024-in-reading</id><content type="html" xml:base="https://conor-hamill.github.io/blog/2024/my-2024-in-reading/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/reading-unsplash-480.webp 480w,/assets/img/reading-unsplash-800.webp 800w,/assets/img/reading-unsplash-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/reading-unsplash.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/reading-coffee-unsplash-480.webp 480w,/assets/img/reading-coffee-unsplash-800.webp 800w,/assets/img/reading-coffee-unsplash-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/reading-coffee-unsplash.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Images taken from <a href="https://unsplash.com">unsplash.com</a>. </div> <h1 id="introduction">Introduction</h1> <p>As 2024 wraps up and I do my best to recover from whatever cold/mild flu is trying to keep me from New Year’s celebrations, I thought it might be a good moment to write up my highlights in things I read this year, across novels, non-fiction, and blogs. I was fortunate enough to hit my Goodreads challenge of 15 books, with a bonus one snuck in as my immune system confined me to the indoors, with a good spread across fiction and non-fiction!</p> <h1 id="fantasy-and-sci-fi">Fantasy and sci-fi</h1> <ul> <li><em>Lyra’s Oxford</em>, Philip Pullman</li> <li><em>Once Upon a Time in the North</em>, Philip Pullman</li> <li><em>A Day of Fallen Night</em>, Samantha Shannon</li> </ul> <p>I started off this year reading the novella’s based in Philip Pullman’s world of <em>His Dark Materials</em> and <em>The Book of Dust</em>, <em>Once Upon a Time in the North</em> and <em>Lyra’s Oxford</em>. I’ve always loved this world but never got round to reading these short books, which served as a nice holdover over until the third and final book in the trilogy gets released, <a href="https://x.com/PhilipPullman/status/1824487662528393639">which I think we can be pretty confident of having a release date next year</a>! Lyra’s Oxford depicts her life in Oxford after the events of <em>The Amber Spyglass</em> in a short story that has some links into <em>The Secret Commonwealth</em>. It expands on the Oxford that exists in the world of <em>His Dark Materials</em>, along with some maps and other documents that flesh out the world a bit more. <em>Once Upon a Time in the North</em> is a short story describing the first meeting of Iorek and Lee Scoresby, two fan favourite characters from <em>His Dark Materials</em>. I liked both of these novellas, but wouldn’t consider either of them mandatory reading for the series as a whole, although <em>Lyra’s Oxford</em> does give a nice introduction to some characters we encounter again in The Secret Commonwealth, and returning to Philip Pullman’s world is always a treat.</p> <p><em>A Day of Fallen Night</em> by Samantha Shannon is a prequel to 2019’s <em>The Priory of the Orange Tree</em>, which introduced her fantasy world of diverse cultures in a “feminist retelling of Saint George and the Dragon”. While I’m probably not the exact target audience for these books, like the previous title, <em>A Day of Fallen Night</em> was a great read. Some of the issues in pacing and characterisation have definitely improved since the last book, and I think this will only get better as Shannon re-visits the rich fantasy world of <em>Roots of Chaos</em> that she’s created.</p> <h1 id="horror">Horror</h1> <ul> <li><em>Pet Sematary</em>, Stephen King</li> </ul> <p>In terms of horror books, I had a fairly limited year, but <em>Pet Sematary</em> was a real stand out among all the books I read. This was a fantastic read and I can see why it’s always highlighted as one of King’s best, so I would definitely recommend to any horror fans or anyone who has enjoyed his other works. This book is definitely up there with some of King’s most distributing, and he recently said himself <a href="https://ew.com/movies/2019/03/29/pet-sematary-stephen-king-interview/">“This is awful. This is really f—ing terrible.”</a>. While I think <em>The Shining</em> is still his scariest book, this definitely stands out as his most horrifying.</p> <h1 id="crime-and-espionage">Crime and espionage</h1> <ul> <li><em>Thirteen</em>, Steve Cavanagh</li> <li><em>Absolute Friends</em>, John le Carre</li> <li><em>Strip Jack</em>, Ian Rankin</li> </ul> <p><em>Thirteen</em> by Steve Cavanagh came with the recommendation of Chris Dillon, and it did not disappoint. A really great thriller and I want to make sure I try more of Cavanagh’s stuff in 2025.</p> <p><em>Absolute Friends</em> by John le Carre was a kind lend from Thomas Lee, who is responsible for introducing me to the works of le Carre. This is definitely one of le Carre’s lesser known novels, covering a friendship across many decades of the 20th century across divided Europe, and feels like one of le Carre’s works that might have a hint of self-insertion in it. While the ending is a little bit out of left field, I would recommend this as one for fans of slow burn espionage to pick up.</p> <p>I finished up <em>Strip Jack</em> recently, and it’s a great entry to the Rebus series, with a tight and focussed story. As one of the earlier Rebus novels, characters like Siobhan and Cafferty are present, but I found I didn’t miss them that much. Would recommend to any Rebus or Scottish crime fans.</p> <h1 id="other-fiction">Other fiction</h1> <ul> <li><em>Watership Down</em>, Richard Adams</li> <li><em>Go Set a Watchman</em>, Harper Lee</li> <li><em>Close to Home</em>, Michael Magee</li> <li><em>Foster</em>, Claire Keegan</li> </ul> <p>With only vague memories of watching the TV show as a child, I found <em>Watership Down</em> a really enjoyable book, that lived up to its reputation of being filled with adventure, great characters, and conflict you wouldn’t expect in a book about rabbits. I was amazed at the depth of the rabbit lore and how well the story stands up, but I’m not sure who I would recommend this to - a bit too horrifying for young children, but maybe not what a lot of adults are looking for, but doesn’t really fit into young adult fiction? Anyway, it doesn’t matter - a great book that remains rightfully considered as a classic.</p> <p>Overall, I think I enjoyed <em>Go Set a Watchman</em> a lot more than the near-universal negative response it got when it was published. It’s definitely a strange book, especially given how established and influential its predecessor is. For me, it seemed to meander quite a bit, then jump to it’s conclusion all of a sudden. It nearly felt like there was another act, or another entire novel, that could have been there to describe how Scout arrived at the final state of her relationship with father. It really does feel like a shame that this is what we got from the Lee estate, but does not diminish any significance of the first book.</p> <p>Close to Home by Michael Magee was a standout novel of the year for me. This semi-autobiographical work based in modern Belfast tackles dealing with the past, finding personal identity, and the working class in Belfast.</p> <p><em>Foster</em> by Claire Keegan was a wonderfully written novella, depicting life in rural Ireland, and I definitely want to read more of her works in 2025.</p> <h1 id="non-fiction">Non-fiction</h1> <ul> <li><em>Monsters</em>, Claire Dederer</li> <li><em>You Don’t Have to Be Mad to Work Here</em>, Benji Waterhouse</li> <li><em>Not the End of the World</em>, Hannah Ritchie</li> <li><em>GCHQ</em>, Richard J. Aldrich</li> <li><em>The Future of Geography</em>, Tim Marshall</li> </ul> <p><em>You Don’t Have to Be Mad to Work Here</em> by Dr. Benji Waterhouse was an enlightening and funny dive into the world of psychiatry, as well as Benji’s personal life. This book had a fantastic opening dedication and I’d hugely recommend.</p> <p><em>Not the End of the World</em> by Hannah Ritchie was another great non-fiction reads of this year, with an optimistic outlook on what we can do to make our planet sustainable. Would give this a wholehearted recommendation for anyone. We were also lucky enough to see Hannah Ritchie present this book at the wonderful <a href="https://www.toppingbooks.co.uk/">Topping Books</a> in Edinburgh, where she was able to answer any and all questions about her research on the spot! She also has a great substack, <a href="https://www.sustainabilitybynumbers.com/?utm_source=substack&amp;utm_medium=web&amp;utm_campaign=substack_profile">Sustainability by Numbers</a>, where she continues to share about the climate and data.</p> <p><em>Monsters</em> by Claire Dederer was a good read that starts off trying to answer the question “how can we enjoy great art made by terrible people?”, a question that has only become more pertinent as the number of artists found guilty of various crimes rises. This book diverted into a more autobiographical work as it went along, and while I’m not sure if it fully answered the question it set out to answer, I enjoyed the analysis of the history of people who have created notable works. The history of the UK’s intelligence, security and cyber agency is covered in <em>GCHQ</em> by Richard J. Aldrich. I learnt a lot about the organisation and its role in the Cold War, with the final 100 pages being the most interesting as GCHQ finds its place in the modern age, but some of the sections were quite dense and weighed down by the amount of information in them.</p> <p><em>The Future of Geography</em> by Tim Marshall was another great work by the author of <em>Prisoners of Geography</em>. The history of the space race during the Cold War and the competition for space in the 21st century will change our world were great reads and I would recommend this to any geography, politics, or space nerd.</p> <h1 id="substacksblogs">Substacks/blogs</h1> <p>This year I got a bit more into Substack, which has become a pleasant spot on the internet for people to write about their niche interests, with authors having control over what content is paid for and readers not being swamped by adverts.</p> <ul> <li><a href="https://www.natesilver.net/">The Silver Bulletin</a>, by Nate Silver <ul> <li>This blog covers analysis on elections, media, and sports and was a great resource for interpreting data during the 2024 US election.</li> </ul> </li> <li><a href="https://verynormal.substack.com/">Very Normal</a> by Christian Pascual <ul> <li>This substack focuses on an approachable manner to teaching concepts statistics, and Christian also has a great <a href="https://www.youtube.com/@very-normal">YouTube channel</a>.</li> </ul> </li> <li><a href="https://www.sustainabilitybynumbers.com/">Sustainability by Numbers</a>, by Hannah Ritchie <ul> <li>I’ve mentioned this above, but this substack focuses on applying data to understand the challenges facing our climate, from the author of <em>Not the End of the World</em>.</li> </ul> </li> <li><a href="https://edinburghminute.substack.com/">The Edinburgh Minute</a>, by Michael MacLeod <ul> <li>This doesn’t quite fit among the other substacks, but for local news in Edinburgh, this daily newsletter can’t be beaten. Highlight recommend.</li> </ul> </li> </ul> <h1 id="some-wonderful-bookshops">Some wonderful bookshops</h1> <p>I also wanted to highlight some of the great bookshops I’ve been to in Edinburgh, Belfast, and Nashville this year, which have fuelled my reading.</p> <ul> <li><a href="https://www.toppingbooks.co.uk/">Topping and Company, Edinburgh</a></li> <li><a href="https://www.theportobellobookshop.com/">The Portobello Bookshop, Edinburgh</a></li> <li><a href="https://rarebirdsbooks.com/pages/visit-us">Rare birds books, Edinburgh</a></li> <li><a href="https://www.parnassusbooks.net/">Parnassus Books, Nashville</a></li> <li><a href="https://noalibis.com/">No Alibis Bookstore, Belfast</a></li> </ul> <p>Here’s to another great year of reading in 2025!</p>]]></content><author><name></name></author><category term="posts"/><category term="books"/><summary type="html"><![CDATA[Some reflections on the books I read this year]]></summary></entry><entry><title type="html">So you want to get into data science?</title><link href="https://conor-hamill.github.io/blog/2024/getting-into-data-science/" rel="alternate" type="text/html" title="So you want to get into data science?"/><published>2024-09-08T17:30:00+00:00</published><updated>2024-09-08T17:30:00+00:00</updated><id>https://conor-hamill.github.io/blog/2024/getting-into-data-science</id><content type="html" xml:base="https://conor-hamill.github.io/blog/2024/getting-into-data-science/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hike-unsplash-image" sizes="95vw"/> <img src="/assets/img/hike-unsplash-image" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dashboard-unsplash-image" sizes="95vw"/> <img src="/assets/img/dashboard-unsplash-image" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Images taken from <a href="https://unsplash.com">unsplash.com </a>. </div> <h2 id="introduction">Introduction</h2> <p>As with all the content on this website and in my blog posts, the opinions expressed here are my own and not those of my employer. More and more often recently, I’ve been asked by students and others “So, what do I need to do to get a job as a data scientist?”. I’ve realised, although I have lots of thoughts on this, the answers I’ve been giving have been rather rambling and unstructured, with more points than any person can reasonably hold on to while talking to me at a careers fair. So I decided to put these thoughts into this short post to bring these notions together in one place. This article will naturally be biased towards someone with a degree in natural sciences (e.g. physics), who has had some experience in programming and statistical analysis, and also inevitably biased towards my personal opinions and recollections.</p> <h2 id="do-you-really-want-to-get-into-data-science">Do you really want to get into data science?</h2> <p>As always before embarking on anything new which will take up a lot of your time, you should yourself two start with two key questions:</p> <ol> <li>Why do I want to do this?</li> <li>Do I really know what I’m getting into?</li> </ol> <p>I highlight these two questions because putting a lot of effort into breaking into data science before realising that it isn’t what you want to do, or it isn’t really what you thought it would be, would be less than ideal. Data science is often not as glamorous as it is depicted, although the perception is now a bit more grounded than it was in the days of “the sexiest job of the 21st century”. As a data scientist, you are required to undertake several different roles at once, stay up to date with a technology landscape that changes extremely quickly, work with requirements that are often unclear and shifting, and, most crucially, you are required to make an impact on the business; to senior management you are a new investment, and they want a return on that investment. Many technology and software-based roles also have these aspects to various degrees, but these are all distinct realities of being a data scientist. Part of this comes from data scientist roles looking different across different organisations and how rapid developments in the world of data and AI has led to high expectations from business leaders. However, my personal opinion is that if you want a job that uses programming and statistical modelling to solve problems, and you want to convince business leaders you’ve solved those problems, you could do far worse than a role as a data scientist.</p> <h2 id="roles-in-data-beyond-data-science">Roles in data beyond data science</h2> <p>A point sometimes neglected is that the volume and variety of data roles beyond that of data scientist are growing year-on-year. As data science practices mature in businesses, the range of professions is increasing, with roles that are more defined than before. This trend seems to closely follow business consensus that you need a much more sophisticated data strategy to really transform a business than just training a model. Some roles, especially data engineers and machine learning engineers, are more sought after than data scientists, and will commonly pay better. There are also roles that require a different range of skills and still provide substantial business value. For example, data analysts can give great insights from data and clearly inform business decision-making, without the need for python and complex algorithms. Additionally, it’s becoming more commonplace for individuals to pivot from related data roles into data science. In short, it will be well worth you’re time exploring which one of these roles you think might best fit your skill set and what you enjoy doing.</p> <h2 id="how-would-i-go-about-learning-data-science-again">How would I go about learning data science again</h2> <p>So, you’ve looked the other roles and decided it’s for you; what do you need to learn and how do you sell yourself? If I was to go back in time and try again to move into data science again, I’d would still follow the same high-level path I took before, with some changes on what I spent the most time on:</p> <ol> <li>Learn the required skills</li> <li>Complete some personal/in work projects to exhibit those skills</li> <li>Advertise those projects to prospective employers</li> </ol> <p>I’ll go through each of these in order in some more detail below.</p> <h3 id="learning-the-skills-you-need">Learning the skills you need</h3> <p>There are a wide range of skills that data scientists need to have, some of which you need to excel at, while knowing the basics of others can suffice. Knowing which of these is which can be tricky.</p> <p>Before picking which resources you want to use to learn, consider what medium you best learn through. I find personally I learn best through textbooks and visual media, but collections of videos seem to be very effective for others. Additionally, be wary of falling into the trap of using bite-sized pieces of knowledge to give yourself the illusion of learning, when you’re really just skimming the surface of the required knowledge and missing deeper connections. Andrej Karpathy described this very well in this <a href="https://x.com/karpathy/status/1756380066580455557?lang=en">recent post on X/Twitter</a>, from which I highlight a paragraph that I find quite impactful:</p> <blockquote> <p>So for those who actually want to learn. Unless you are trying to learn something narrow and specific, close those tabs with quick blog posts. Close those tabs of “Learn XYZ in 10 minutes”. Consider the opportunity cost of snacking and seek the meal - the textbooks, docs, papers, manuals, longform. Allocate a 4 hour window. Don’t just read, take notes, re-read, re-phrase, process, manipulate, learn.</p> </blockquote> <p>     Andrej Karpathy, <a href="https://x.com/karpathy/status/1756380066580455557?lang=en">Twitter/X</a></p> <p>Below I describe roughly the order I would undertake learning, assuming a prior degree in some sort of science. Many of these resources are described in my <a href="/data_science_resources/">page on data science resources</a>.</p> <ul> <li>Learn software development: <ul> <li>with <a href="https://missing.csail.mit.edu/">the missing semester course from MIT</a>. <ul> <li>this might feel like a dry one to start off with, but since this is probably the most fundamental skill here and all others rely on it, it is a good place to start. Being comfortable with you terminal and being able to collaborate using Git will make future learnings and your eventual job much easier.</li> </ul> </li> </ul> </li> <li>Learn python: <ul> <li>I don’t have an exact python course I would recommend and given there are many different levels you may be starting at, I would steer you towards the <a href="https://realpython.com/learning-paths/">choice of python learning paths on Real Python</a>.</li> <li>The main python website <a href="https://www.python.org/about/gettingstarted/">python.org</a> has also indexed many useful resources <ul> <li>Out of all the skills here, python is the one you probably want to put the most time into being good at, because it will be the end product you will be producing a lot of the time.</li> </ul> </li> </ul> </li> <li>Learn statistical modelling: <ul> <li>The book “An Introduction to Statistical Learning” is available with examples in R or Python <a href="https://www.statlearning.com/">here</a>. <ul> <li>I’ve found this book to cover the level of detail needed to understand modelling as a data scientist in a very digestible way.</li> </ul> </li> </ul> </li> <li>Learn how to do a machine learning project: <ul> <li>This point has a lot of similarities with the above, but I think this one begins the focus on the python packages used more</li> <li>Andrew Ng’s <a href="https://www.deeplearning.ai/courses/machine-learning-specialization/">machine learning specialisation</a> on Coursera remains a classic for a reason, although pytorch has established itself over Tensorflow as the python deep learning framework of choice</li> <li>For a book alternative, I would recommend <a href="https://sebastianraschka.com/blog/2022/ml-pytorch-book.html">“Machine Learning with PyTorch and Scikit-Learn”</a></li> </ul> </li> <li>Learn (some) cloud computing: <ul> <li>How much you need to know will vary role by role, but understanding the fundamentals is good.</li> <li>I don’t have a stand-out recommendation for learning cloud computing, but have had a good experience learning with Udemy’s courses on AWS in the past. This <a href="https://www.udemy.com/course/introduction-to-cloud-computing-on-amazon-aws-for-beginners/">one may be a good introduction</a> for those new to AWS or cloud computing.</li> </ul> </li> </ul> <p>After (or maybe while) you have learnt the above skills, you can begin to undertake some personal projects.</p> <h3 id="doing-some-personal-projects">Doing some personal projects</h3> <p>You need to show that you can use your skills to solve problems and explain why you’ve made the choices that you have. Picking a good personal project can be tricky; you want something that is complex enough to challenge yourself, but not something that isn’t fundamentally unfixable. Remember that training a good machine learning model is only one of the components of providing a business-ready solution - if you are able to implement an end-to-end solution, possibly deploying in the cloud, that can be very impressive (but do remain vigilant of AWS costs piling up rapidly!).</p> <p>Do something that is either relevant to an industry you’re interested in or that you’re personally interested in. This makes it more likely the problem and solution will be more relevant to roles you’re looking for and more interesting all round. Alternatively, if you can apply some data science to your current role, that is an excellent way to show you can take initiative and solve problems.</p> <h3 id="advertise-these-projects-to-potential-employers">Advertise these projects to potential employers</h3> <p>Finally, you want to make sure that potential recruiters will see these projects, and you can set yourself apart from other applicants. Put the code that you have developed in a public GitHub repository and make sure it has a story behind it - maybe you want to write a blog about it, or make a really interesting readme for the project, or find some way to host it online for others to use! Whatever way you decide to advertise it, make sure it clearly shows how you’ve solved your problem and links to your code. Additionally, do spend time tuning your CV to sell yourself and make sure it’s specific to whatever job you are applying to.</p> <h2 id="conclusions">Conclusions</h2> <p>That wraps up some pointers for how I would go about trying to get into data science if I had to again. Bear in mind this guide is very much not end-to-end and many of the soft skills that are essential for a data science are not given the time they are due in this post. Some resources below include alternative opinions that might be useful.</p> <h2 id="resources">Resources</h2> <ul> <li><a href="https://www.reddit.com/r/datascience/comments/qph4tx/how_to_get_a_job_in_data_science_a_semiharsh_qa/">How to get a job in data science - a semi-harsh Q/A guide</a> - many of the sentiments in this article are echoed in this post from u/save_the_pandas_bears, albeit in some stronger tones</li> <li><a href="https://www.datacamp.com/blog/how-to-become-a-data-scientist">How to become a Data Scientist in 2024 - Datacamp</a></li> <li><a href="https://shecancode.io/blog/can-i-get-a-data-science-job-with-no-prior-experience/">“Can I get a data science job with no experience?” - SheCanCode</a></li> </ul>]]></content><author><name></name></author><category term="posts"/><category term="data-science"/><summary type="html"><![CDATA[Some advice on how I would go about moving into data science if I had to do it again]]></summary></entry><entry><title type="html">What I would tell myself at the start of my PhD</title><link href="https://conor-hamill.github.io/blog/2024/what-I-would-tell-myself-phd/" rel="alternate" type="text/html" title="What I would tell myself at the start of my PhD"/><published>2024-06-18T17:30:00+00:00</published><updated>2024-06-18T17:30:00+00:00</updated><id>https://conor-hamill.github.io/blog/2024/what-I-would-tell-myself-phd</id><content type="html" xml:base="https://conor-hamill.github.io/blog/2024/what-I-would-tell-myself-phd/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nebula-image-480.webp 480w,/assets/img/nebula-image-800.webp 800w,/assets/img/nebula-image-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/nebula-image.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/books-image-480.webp 480w,/assets/img/books-image-800.webp 800w,/assets/img/books-image-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/books-image.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Images taken from <a href="https://unsplash.com">unsplash.com</a>. </div> <h2 id="introduction">Introduction</h2> <p>Completing my PhD in nuclear astrophysics in 2021 was the end of an experience filled with both immense challenges and rewarding experiences. After this, I transitioned from academia to industry and have worked for three years as a data scientist in financial services. I really enjoyed my PhD (despite the start of it being quite challenging and the pandemic making the end a bit of a damp squib) and it led to a very satisfying job in a team I really enjoy being part of, but there are lots of things I wish I had said to myself or had done differently during my PhD. One of the perennial problems in academia is that the knowledge and experiences of people in research groups are often lost as they move to different roles in different institutions. This happens very commonly with departing PhD students; the people who were once a major component of a research group are now only remembered by their names on the side of old theses on an office shelf. Given this, and since I’ve been away from my PhD long enough to see the world outside academia and reflect on things, compiling advice to give my past self seemed like a good activity.</p> <p>I started my PhD in nuclear astrophysics at the University of Edinburgh after completing my integrated master’s at Queen’s University Belfast, in a different sub-field to that of my master’s project. After submitting my thesis I got a job in a large retail bank as a data scientist, where I still work. The thoughts in this article will be specific to these experiences, and might not be directly applicable to those in other fields, programmes, or who continued their academic career, but hopefully some parts will be of use to anyone undertaking a PhD. This blog post contains the advice I would give to myself at the start of my PhD, to provide a little bit of help navigating the highs and lows. Whether you’re just starting out or knee-deep in your research, I hope you find something here that is of some use, or at least resonates with you.</p> <h2 id="get-organised-and-learn-the-skills-you-will-need-from-the-start">Get organised and learn the skills you will need from the start</h2> <h4 id="stay-organised">Stay Organised</h4> <p>Taking steps to be organised from the get go will serve you well as you establish yourself as a reliable department member and reduce the cognitive load of juggling things in your mind. The week-to-week rhythm of a PhD programme is much less structured than undergrad programmes, so keeping track of a wide range of events may be somewhat new. Whether with a physical diary or a virtual calendar, use whatever tools you need to keep yourself accountable and remember meetings, deadlines, and appointments.</p> <h4 id="learn-to-code-efficiently">Learn to Code Efficiently</h4> <p>Something that I think is chronically underrated in most PhD programmes is learning how to use your terminal and write high-quality code. A large component of the day-to-day work you will be doing will be coding and will likely be one of the most important skills for future employers, so learning to do this well early on and continuing to actively improve during your programme will serve you well. There are many many courses out there for learning your terminal and every programming language, but one course I think that should be mandatory is the <a href="https://missing.csail.mit.edu/">missing semester course from MIT/</a>; this course covers using the command line, using your editor efficiently, and gives a good overview of how you can make your workflow more efficient and your life easier. My undergraduate programme covered a wide range of physics topics very well, but a lot of what I learnt about the command line and the surrounding ecosystem came from outside my course, so this would have really helped early in my PhD. Something essential that is covered in that course is version control using Git, a tool that is ubiquitous across any role that involves programming and something that will make controlling the code (and manuscripts) you write a lot easier. This can be skipped in undergraduate courses but is a skill everyone wishes they had learnt earlier.</p> <h4 id="choose-the-best-tools-for-the-job">Choose the best tools for the job</h4> <p>You will also need to choose what IDE and reference manager you will be using. Spending a bit of time choosing which of these you prefer can be worth it, given how much time you will be spending using them. For IDEs, <a href="https://www.jetbrains.com/pycharm/">pycharm</a> or <a href="https://code.visualstudio.com/">VSCode</a> are popular choices for a reason, and the reference manager Zotero allows article organisation, sharing across devices, and automatic extraction of information, making it a useful tool.</p> <h4 id="leverage-the-past-experiences-of-others">Leverage the past experiences of others</h4> <p>Do also ask older students (who will be a great source of information) what tools they’ve found to be useful and what resources they’ve found useful for learning programming - people who have recently walked the path you’re on will know best.</p> <h2 id="dont-compare-yourself-against-other-people">Don’t compare yourself against other people</h2> <p>Comparison is always the thief of joy, and never more so than during a PhD. In one sentence: do not compare your progress, skills, or achievements to others in your group or anyone else doing a PhD. This will simply make you unhappy as you form a habit of picking apart everything that hasn’t gone amazingly. In addition, it is an objectively terrible metric to measure your progress against. There are several reasons for why this simply isn’t a good measure of how you’re progressing, but what’s key to remember is that the structure from the undergrad degree where everyone takes the same lectures and exams at the same time is no longer there. First, your colleagues will have come from different universities, which may have focussed more or less on skills like programming, or may have specialised in your particular PhD field, meaning everyone is at a different starting point. Second, everyone’s project is different - some might be more theoretical, some might be part of a larger collaboration, while some may focus on one specific device; everyone’s PhD project is unique. Thirdly, there are simply some things that will be outside your control. This will include experiment scheduling, the work of collaborators, and academic politics.</p> <h3 id="focus-on-your-own-path">Focus on your own path</h3> <p>Concentrate on the things that you can control day-to-day and don’t feel that your colleague’s breakthrough, publication, or talk is a sign you’re falling behind or that you’re not cut out for things. Celebrate each and every milestone, as minor as they may be, and, whatever pace you’re moving at, give yourself credit for every step you move forward.</p> <h2 id="treat-it-like-a-job">Treat it like a job</h2> <p>Doing a PhD is full of contradictions: you’re still a student, but getting paid; you have lots of work to be doing, but often no defined working hours; you’re still learning, but considered an expert in your field. While you might not ever have flexibility like this again in your career, treating your PhD like a job with a regular 9-5 job, with clearly defined boundaries between work and time off, is likely the best to do for your productivity and well being. Waking up at the same every day will help you control your energy levels while giving you time to turn off from work. Keeping the work-life boundary is also important and going into your office and not checking emails when at home will be a key part of this. Finally, you’ll still have peers who want to head out throughout the week, so it’s up to you to enjoy yourself while balancing where your energy goes.</p> <h2 id="engage-with-your-department-and-community-effectively">Engage with your department and community effectively</h2> <h3 id="interact-with-everyone">Interact with everyone</h3> <p>There will likely be no other point in your life that you’re in an environment with such smart and hard-working people who are the top experts in your field. Get to know them and their experiences and learn everything you can from them. Part of this is going into the office, chatting during coffee breaks and lunch and being genuinely interested in their research. Additionally, if there are internal presentations or reading groups, do go to those, make your own contributions, and if you don’t have these, make a proposal to senior members of your department about what they could look like and why you should have them. Older students are great sources of information and being able to passively pick up advice from them is invaluable, but this will often only happen if you see them in-person.</p> <h3 id="keep-communication-with-your-supervisor-strong">Keep communication with your supervisor strong</h3> <p>Supervisors have vastly different rhythms for communicating with their students and especially for those that are hands off, flying under the radar can easily happen. My suggestion to avoid this is to communicate regularly and in-detail with them so anything that’s a problem is highlighted as early as possible, and they have a clear view of how your research is going. One way of doing this might be a weekly email that describes what you did that week, what you found out, what you found challenging, and anything that you think is an issue. As well as making sure your supervisor is abreast of everything happening in your research, it means that you will get the satisfaction of seeing your work progress week-to-week and have a record of everything you’ve achieved.</p> <h2 id="keep-non-phd-interests">Keep non-PhD interests</h2> <p>Like the muscles in your body, your brain needs to relax every so often or it will begin to be strained. Making time for your non-PhD interests from week-to-week is a great way of doing this and not letting your PhD work consume your thoughts and identity. Additionally, make sure part of this non-PhD time is dedicated to physical activity, be it team sports, the gym, or walking.</p> <h2 id="people-will-not-understand-what-youre-doing-or-why-youre-doing-it">People will not understand what you’re doing or why you’re doing it</h2> <p>You will likely get a wide variety of reactions whenever you tell people that you’re doing a PhD. Some will proclaim you a genius for starting a programme (which you should still be proud of), while some will be suggesting you’re putting off the world of work, or lying in bed all day. Both ends of this spectrum of this reaction can exacerbate imposter syndrome - it can be common to not feel as smart as others might describe you and you also might feel that you indeed haven’t made much progress in things. It can be frustrating and lonely that others don’t understand the nature of the challenges a PhD entails, and describing these challenges can be difficult in itself. But the truth is that very few people will understand the nature of your work, or your exact experience going through it, as, by its very nature, you are covering uncharted territory. So learn to shrug off the sceptics and accept the faith that people have in your ability, and learn your one sentence lay-person description of your work that doesn’t exact capture what you’re doing, but is close enough and sounds a little bit interesting: “I’m doing experiments to help find out where all the elements in our Galaxy come from”.</p> <h2 id="nailing-the-landing-is-difficult-if-not-impossible">Nailing the landing is difficult, if not impossible</h2> <h3 id="plan-for-the-future">Plan for the future</h3> <p>While it may seem like a lifetime away whenever you begin your programme, your PhD studies will come to an end at some point. The end of the PhD will quite often require some multi-tasking: wrapping up final data analysis, applying for new roles, and of course, writing your thesis. This is inevitably a difficult time, but there are some steps you can take to try to make this time as straightforward as possible.</p> <h3 id="career-preparation">Career preparation</h3> <p>The foremost tip I would have is to give some thought to next career steps in your career early on in your PhD, possibly as early as your second year. Don’t worry if you don’t know immediately what it is you want to and it’s perfectly fine to change your mind, but the earlier you think about this, the earlier you can take steps to make the end of your PhD easier. Take an industry placement if you can: this will give you exposure to the world outside academia, providing you with new contacts and skills, and something to differentiate your CV.</p> <h3 id="coding-skills-for-industry">Coding skills for industry</h3> <p>For anyone planning to get a job in industry after their PhD, the one bit of definite advice I would give would be: <strong>learn to code and learn to code well</strong>. This will be very likely what you will be doing in industry, so the sooner you get better at it, the sooner you can market your skills well. Also think about the programming language that you’ve used in your PhD; if it is a proprietary language or one that’s quite niche to academia, consider investing time in learning a more common language, or advertising the general programming skills you’ve acquired (object-oriented programming, multi-core programming). Also remember that recruiters and employers will often not speak the same language as you - having a well-cited paper is impressive and a strong marker of success in academia, but describing the more practical results of code you wrote or actions you took will land more strongly with employers. Also remember that you have acquired a lot of skills in your PhD, by working in a team, presenting and justifying results, and managing your own time, but making sure these are communicated in a way that industry can interpret them is essential to making an impact in the job market.</p> <h2 id="writing-your-thesis">Writing your thesis</h2> <p>I also need to echo what many more have said on having perfection as the enemy of done when it comes to writing a PhD thesis. It is natural to feel attached to the quality of this book you have written, after all it is a culmination of many years of work and learning. However, it is worth bearing in mind that it is a means to an end: a document to show your assessors that you have done novel research and are deserving of your degree. As the saying goes, “There is no such thing as a perfect thesis, only a done thesis”. In addition, the writing of your thesis will take at least twice as long as you think, no matter how you go about it, which is something to bear in mind whenever you are planning when to start writing and when to tell an employer you can start.</p> <h2 id="leaving-your-programme-is-not-the-end-of-the-world">Leaving your programme is not the end of the world</h2> <p>Finally, I want to say that if you do decide that you do leave your PhD programme for whatever reason, you are no way in the slightest a failure and this does not define your worth or success. The truth is, in general, no one cares a huge deal if you have a PhD or not. When you start a job, you are known by your job title, not the letters before or after your name. Leaving a PhD can often be the correct decision; in the end we have to weigh up the rewards and efforts of every endeavour in life, and if the scales tilt in the negative direction, then it simply isn’t worth persisting. If you feel like people will be disappointed in you for quitting, then doing a PhD to impress them wasn’t the correct choice in the first place. For an employment perspective, you still have each and every one of the skills you developed over the course of your PhD. Having the final degree certificate doesn’t make a difference in how well you undertake your next role.</p> <h2 id="conclusion">Conclusion</h2> <p>These were some of my thoughts I had when reflecting back on my PhD experience. Hopefully they ring true for some others and are useful to those starting or mid-way through their PhD journeys. Overall, remember that everyone’s PhD journey is unique and not comparable to others, make sure you have a life and interests outside your PhD, think (but don’t worry) about the future, learn to code, and accept your thesis may not be the immaculate masterpiece you set out to create (and that’s perfectly fine).</p>]]></content><author><name></name></author><category term="posts"/><category term="phd"/><summary type="html"><![CDATA[Some reflections and advice I would give myself starting a PhD]]></summary></entry><entry><title type="html">Visualisation libraries in python for expanding your data storytelling</title><link href="https://conor-hamill.github.io/blog/2024/visualisation-libraries-in-python-for-expanding-your-data-storytelling/" rel="alternate" type="text/html" title="Visualisation libraries in python for expanding your data storytelling"/><published>2024-03-30T11:41:15+00:00</published><updated>2024-03-30T11:41:15+00:00</updated><id>https://conor-hamill.github.io/blog/2024/visualisation-libraries-in-python-for-expanding-your-data-storytelling</id><content type="html" xml:base="https://conor-hamill.github.io/blog/2024/visualisation-libraries-in-python-for-expanding-your-data-storytelling/"><![CDATA[<h3>5 python packages for expanding your data storytelling</h3> <p>Data visualisation is a core part of how a data scientist tells a story, encompassing how they explore their data, share insights, and explain the impact of their models in their specific domain. The three most popular python libraries for visualisation are <a href="https://matplotlib.org/">matplotlib</a>, <a href="https://seaborn.pydata.org/">seaborn</a>, and <a href="https://plotly.com/">plotly</a>, with many other frameworks allowing the creation of impactful plots, like <a href="https://altair-viz.github.io/">Altair</a>, <a href="http://bokeh.org/">Bokeh</a>, and <a href="https://yhat.github.io/ggpy/">ggplot</a>.</p> <p>However, the core bread-and-butter python libraries that make up the skillsets of (many) modern data scientists are based around pandas, for data manipulation and analysis; scikit-learn, for pre-processing and modelling; and matplotlib for visualisation, respectively. Being able to expand upon these frameworks without having to learn a new framework and syntax allows a lot of impact with little effort and re-skilling.</p> <p>Inspired by this thought, this article explores five visualisation libraries that focus on expanding the capabilities of these core packages, including some thoughts on how well they deliver on their goals and which ones are worth adding to your arsenal. The five data visualisation libraries covered will be <a href="https://www.scikit-yb.org/en/latest/index.html">Yellowbrick</a>, <a href="https://rasbt.github.io/mlxtend/">Mlxtend</a>, <a href="https://github.com/matplotlib/mplfinance">mplfinance</a>, <a href="https://mpld3.github">mpld3</a>, and <a href="https://posit-dev.github.io/great-tables/articles/intro.html">great_tables</a>.</p> <h3>Yellowbrick</h3> <ul><li>Docs: <a href="https://www.scikit-yb.org/en/latest/index.html">https://www.scikit-yb.org/en/latest/index.html</a></li></ul> <p>The summary on its website gives a concise description of the package: <strong>“Yellowbrick extends the Scikit-Learn API to make model selection and hyperparameter tuning easier. Under the hood, it’s using Matplotlib”</strong>. It includes visualisations for many aspects of machine learning, including feature analysis, regression, classification, and clustering. Each of the visualisers on offer is implemented as a function and as a class, which simply wraps the function.</p> <p>To experiment with how flexible this would be with matplotlib subplots, I took the California Housing toy dataset from scikit-learn, for regression problems, and made a single figure with plots that show both the fit of the model and the distribution of the residuals of the fit. The visualisers can take scikit-learn pipelines as inputs, as well as classifiers, making reuse of scikit-learn code convenient.</p> <p>While I wasn’t able to get plotting on multiple matplotlib axes objects using the class-based API for visualisers, the “quick draw” functions shown below worked well, producing a plot that showed my regression fit, while also quickly showing that my residuals weren’t ideally distributed for a linear regression model. This implementation is shown in the code block below.</p> <pre>from sklearn.datasets import fetch_california_housing<br />import pandas as pd<br /><br />from sklearn.pipeline import make_pipeline<br />from sklearn.model_selection import train_test_split<br />from sklearn.preprocessing import StandardScaler<br />from sklearn.linear_model import LinearRegression<br />from sklearn.metrics import r2_score<br />import matplotlib.pyplot as plt<br /><br />from yellowbrick.regressor.prediction_error import prediction_error<br />from yellowbrick.regressor import residuals_plot<br /><br /># Getting example dataset<br />X, y = fetch_california_housing(return_X_y=True, as_frame=True)<br />X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)<br /><br />feature_cols = [<br />    &quot;MedInc&quot;,<br />    &quot;HouseAge&quot;,<br />    &quot;AveRooms&quot;,<br />    &quot;AveBedrms&quot;,<br />    &quot;Population&quot;,<br />    &quot;AveOccup&quot;,<br />]<br /><br />pipeline = make_pipeline(<br />    StandardScaler(),<br />    LinearRegression(),<br />)<br /><br />pipeline.fit(X_train[feature_cols], y_train)<br /><br />fig, axs = plt.subplots(2, 1, figsize=(14, 12));<br /><br /># quick draw method<br />visualiser_errors = prediction_error(<br />    pipeline,<br />    X_train[feature_cols],<br />    y_train,<br />    X_test[feature_cols],<br />    y_test,<br />    show=False,<br />    shared_limits=False,<br />    ax=axs[0],<br />)<br /><br /># quick draw method<br />visualiser_residuals = residuals_plot(<br />    pipeline,<br />    X_train[feature_cols],<br />    y_train,<br />    X_test[feature_cols],<br />    y_test,<br />    hist=False,<br />    qqplot=True,<br />    show=False,<br />    ax=axs[1],<br />)<br /><br />visualiser_errors.ax.set_xlim([0, 6])<br />visualiser_errors.ax.set_ylim([0, 8])<br />visualiser_errors.ax.legend(loc=2)<br /><br />fig</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bgjf5RZJTe83bmrycYNpQA.png"/><figcaption>Plot fit and residuals for linear regression model on the California Housing dataset, visualised using Yellowbrick.</figcaption></figure> <p>The plot above gives the insight I was looking for and only required two function calls to plot (plus some formatting).</p> <p><strong>My take: </strong>this package contains lots of functionality for informative visualisation, while allowing the reuse of scikit-learn pipelines and formatting using object-orientated matplotlib. I’d recommend this to any data scientist looking for a quick and easy way to visualise their models.</p> <h3>Mlxtend</h3> <ul><li>Docs: <a href="https://rasbt.github.io/mlxtend/">https://rasbt.github.io/mlxtend/</a></li></ul> <p>This package from the well-known data science researcher, author, and developer of <a href="https://lightning.ai/">PyTorch Lightning</a>, <a href="https://sebastianraschka.com/">Sebastian Raschka</a>, introduces itself as “<strong>Mlxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks”. </strong>While not exclusively focussed on visualisation, it contains some visualisation options, with matplotlib as the backend.</p> <p>To give this package a spin, I decided to use the plot_decision_region() function, to compare the decision regions of the logistic regression, SVC, and adaboost classifiers on the penguins dataset. The code below shows the implementation of the plotting of three sub-plots, with an accuracy for each classifier as the title of the subplots.</p> <pre>from sklearn.linear_model import LogisticRegression<br />from sklearn.svm import SVC<br />from sklearn.ensemble import HistGradientBoostingClassifier, AdaBoostClassifier<br />from sklearn.impute import SimpleImputer<br />from sklearn.preprocessing import OneHotEncoder, LabelEncoder<br />from sklearn.compose import ColumnTransformer<br />from sklearn.metrics import accuracy_score<br />import numpy as np<br />from mlxtend.plotting import plot_decision_regions<br /><br /># penguins dataset downloaded locally<br />penguin_filepath = &quot;../data/penguins_size.csv&quot;<br />df_penguins = pd.read_csv(penguin_filepath)<br /><br />penguin_feature_cols = [<br />    &quot;culmen_length_mm&quot;,<br />    &quot;culmen_depth_mm&quot;,<br />    # &#39;flipper_length_mm&#39;, &#39;body_mass_g&#39;,<br />]<br /><br />le = LabelEncoder()<br /><br />X = df_penguins[penguin_feature_cols]<br />y = le.fit_transform(df_penguins[&quot;species&quot;])<br />X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)<br /><br />simple_inputer = SimpleImputer()<br />X_train = simple_inputer.fit_transform(X_train)<br />X_test = simple_inputer.transform(X_test)<br /><br />lr_model = LogisticRegression().fit(X_train, y_train)<br />svc_model = SVC().fit(X_train, y_train)<br />adab_model = AdaBoostClassifier().fit(X_train, y_train)<br /><br /># plotting<br />figure, ax = plt.subplots(1, 3, figsize=(16, 6))<br /><br />clf_names = [&quot;Logistic regression&quot;, &quot;SVC&quot;, &quot;AdaBoost&quot;]<br />clfs = [lr_model, svc_model, adab_model]<br /><br />xlabel, ylabel = penguin_feature_cols<br />for clf, clf_name, ax in zip(clfs, clf_names, axes):<br />    plot_decision_regions(X_test, y_test, clf=clf, ax=ax)<br /><br />    acc = accuracy_score(y_test, clf.predict(X_test))<br /><br />    ax.set_xlabel(xlabel)<br />    ax.set_ylabel(ylabel)<br />    ax.set_title(f&quot;{clf_name}: accuracy: {100.0 * acc:.2f}%&quot;)<br /><br />figure</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*G3oAAciRBEtMQhgwADBEZA.png"/><figcaption>Decision regions for logistic regression, SVC, and adaboost classifiers on penguins dataset, visualised using the Mlxtend package.</figcaption></figure> <p>Relatively easily, we are able to plot the decision regions for each of these classifiers on the axes objects. These plots show how each of these classifiers separate predictions based on features, including exhibiting the overfitting of the adaboost classifier. We did have to convert the data to numpy arrays, as this function doesn’t accept pandas dataframes as input, despite pandas now accepting dataframes as input for transformers. Given how busy Sebastian Raschka has been with his other projects, I don’t think we can be too harsh about this.</p> <p><strong>My take: </strong>This package has plotting functionality that allows the user to get further insights into their models by re-using their scikit-learn models and enabling the use of matplotlib to configure the plots as you wish. The other functionality offered by the package is also definitely worth exploring.</p> <h3>mplfinance</h3> <ul><li>Docs: <a href="https://github.com/matplotlib/mplfinance">https://github.com/matplotlib/mplfinance</a></li></ul> <p>This package describes itself as <strong>“matplotlib utilities for the visualization, and visual analysis, of financial data”</strong>. The README in the GitHub repo describes the history of the package, which began as code extracted from the depreciated matplotlib.finance package, and had a previous form under the name mpl-finance. The package focuses on the visualisation of financial data (e.g. stock prices) over time and provides several examples of how to do this in its <a href="https://github.com/matplotlib/mplfinance/tree/master/examples">examples directory</a>.</p> <p>To give this package a try, I downloaded a sample of Apple’s stock price and plotted a candlestick plot for a sample of the stock price back in 2004.</p> <pre>import mplfinance as mpf<br />import matplotlib.pyplot as plt<br /><br />df_AAPL = pd.read_csv(&quot;../data/AAPL.csv.zip&quot;)<br /><br />df_AAPL.set_index(&quot;Date&quot;, inplace=True)<br />df_AAPL.index = pd.to_datetime(df_AAPL.index)<br />df_AAPL = df_AAPL[(df_AAPL.index &gt; &quot;2004-02-01&quot;) &amp; (df_AAPL.index &lt; &quot;2004-05-01&quot;)]<br /><br />mpf.plot(df_AAPL, type=&quot;candle&quot;, style=&quot;binance&quot;)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*93uq1AAk-EZSm3_fL4ZVcA.png"/><figcaption>Apple stock in early 2004, plotted using mplfinance.</figcaption></figure> <p>This shows a candlestick plot that looks decent, with one line of plotting. Provided the dataframe passed has the correct headings and the index set as the date, mplfinance can pick up the details it needs to create the plot.</p> <p>The package also has the option to add more information on the daily stock prices to the plots. Using the same data, I used the addplot option of mpl.plot to display the high and low values for each day in one subplot, above a subplot that showed the volume of stock traded every day and the range every day.</p> <pre>df_AAPL[&quot;range&quot;] = df_AAPL[&quot;High&quot;] - df_AAPL[&quot;Low&quot;]<br /><br />add_plots = [<br />    mpf.make_addplot(df_AAPL[[&quot;High&quot;, &quot;Low&quot;]]),<br />    mpf.make_addplot(df_AAPL[&quot;range&quot;], panel=1, color=&quot;g&quot;),<br />]<br />mpf.plot(df_AAPL, addplot=add_plots, volume=True)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-MMwHE-NnJr7MRm-ODWe4w.png"/><figcaption>High and low values for Apple’s stock price in early 2004, above the volume traded and range for the stock, plotted using mpl.finance.</figcaption></figure> <p>This plot contains a lot of information, created with relatively few lines of code. However, the right-hand side y-axis on the lower plot is missing a label. Quite often, plotting functions like this will return the figure or axes objects automatically. However, in the doc string for the plot() function, there is no description of how to access this, or what the function can return. I dug into the source code and found that some of the kwargs provided to the function do have effects, including returnfig=True returning the figure and axes objects. From this, I was able to access the axis object I needed and add a label, but having to dig to find this functionality was fairly inconvenient.</p> <pre>fig, *axes_objects = mpf.plot(df_AAPL, addplot=add_plots, volume=True, returnfig=True)<br />axes_objects[0][3].set_ylabel(&quot;Range&quot;)<br /><br />fig</pre> <p><strong>My take: </strong>This package does enable visualisation of financial data in few lines of code. However, it is let down by the user having to really dig in to find the full functionality, and the slightly strange make_addplot API. That being said, this package has a lot of potential and the number of examples and plotting styles are good. A possible alternative for users would be the <a href="https://plotly.com/python/candlestick-charts/">implementation of candlestick charts in plotly</a>, which has the strong advantage of being able to zoom in on specific ranges in a long time series.</p> <h3>mpld3</h3> <ul><li>Docs: <a href="https://mpld3.github">https://mpld3.github</a></li></ul> <p>This package <strong>“brings together </strong><a href="http://www.matplotlib.org/"><strong>Matplotlib</strong></a><strong>…and </strong><a href="http://d3js.org/"><strong>D3js</strong></a><strong>, the popular JavaScript library for creating interactive data visualizations for the web”</strong>, setting out to tackle one of matplotlib’s biggest weaknesses - its lack of interactivity. The docs contain some nice examples of how this can be achieved in matplotlib plots, along with exporting the resulting plots as html.</p> <p>I used the penguins dataset again to give this library a try. One of the common problems when visualising large datasets is having overlapping distributions of several groups, making it difficult to observe and compare individual distributions. After trying to replicate the interactive legend example for histograms for each group, I had no success. After some searching, I concluded the containers that are used for histograms aren’t supported by mpld3, but I’d be delighted to be shown I’m wrong. Fortunately, I was able to get the scatter plot functionality working for the penguins dataset, giving me some information about the distribution of the two features, which would be very useful on a larger dataset.</p> <pre>fig, ax = plt.subplots(figsize=(10, 6))<br /><br />ax.grid(True, alpha=0.3)<br /><br />for species in df_penguins[&quot;species&quot;].unique():<br />    (l,) = ax.plot(<br />        df_penguins[df_penguins[&quot;species&quot;] == species][&quot;culmen_depth_mm&quot;].values,<br />        df_penguins[df_penguins[&quot;species&quot;] == species][&quot;culmen_length_mm&quot;].values,<br />        label=species,<br />        marker=&quot;.&quot;,<br />        markersize=20,<br />        linestyle=&quot;None&quot;,<br />    )<br /><br />handles, labels = ax.get_legend_handles_labels()  # return lines and labels<br />interactive_legend = plugins.InteractiveLegendPlugin(<br />    ax.lines, labels, alpha_unsel=0.1, alpha_over=1.5, start_visible=True<br />)<br /><br />plugins.connect(fig, interactive_legend)<br /><br />ax.set_xlabel(&quot;Culmen depth [mm]&quot;)<br />ax.set_ylabel(&quot;Culmen length [mm]&quot;)<br />ax.set_title(&quot;Culmen length vs depth&quot;, size=20)<br /><br />mpld3.display(fig)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*I6-7z_AaeW1R5y2qaNxuag.png"/></figure> <p>The above picture obviously doesn’t display the functionality, but by clicking on the legend in the notebook the individual groups will disappear, and this can also be exported to a html file. Also, before the plugins from mpld3 get used, no changes to the code are needed, meaning existing matplotlib code can be easily extended to give this functionality.</p> <p><strong>My take: </strong>This package makes a solid attempt towards adding interactivity to matplotlib, and has an appealing syntax in that existing matplotlib scripts can have a few lines of code appended to give interactivity. However, it doesn’t work for all types of matplotlib plots, and <a href="https://mpld3.github.io/faq.html#general">judging by the FAQ</a>, adding interactivity simply won’t be possible for some types of plots, meaning following a plotly example might be your best bet if you’re starting from scratch and want interactivity.</p> <h3>great_tables</h3> <ul><li>Docs: <a href="https://posit-dev.github.io/great-tables/articles/intro.html">https://posit-dev.github.io/great-tables/articles/intro.html</a></li></ul> <p>While displaying data in tabular format might not be the first thing people think of when they hear data visualisation, a well-structure table of aggregated data in a table can be an effective way of displaying information for a report or presentation, and being able to generate these tables automatically makes life much easier. This package introduces itself as <strong>“a Python package for creating great-looking display tables”</strong>, and makes the distinction that this is aimed to offer features to create tables “you’d find in a web page, a journal article, or magazine”.</p> <p>To give this package a go and see how pretty a table I could create, I downloaded a table of New York property prices from Kaggle and set about displaying some aggregate properties for each area, included in the code snippet below.</p> <pre>from great_tables import GT<br />from great_tables import html, md<br /><br />import calendar<br /><br />df_NY = pd.read_csv(NY_housing_path)<br /><br />NO_BEDS_COL = &quot;No. beds&quot;<br />NO_BATHS_COL = &quot;No. baths&quot;<br />FLOOR_AREA_COL = &quot;Floor area [sqft]&quot;<br />NO_LISTINGS_COL = &quot;No. listings&quot;<br />PRICE_COL = &quot;Price&quot;<br />LOCALITY_COL = &quot;NY Locality&quot;<br /><br />rename_dict = {<br />    &quot;LOCALITY&quot;: LOCALITY_COL,<br />    &quot;BEDS&quot;: NO_BEDS_COL,<br />    &quot;BATH&quot;: NO_BATHS_COL,<br />    &quot;PROPERTYSQFT&quot;: FLOOR_AREA_COL,<br />    &quot;ADDRESS&quot;: NO_LISTINGS_COL,<br />    &quot;PRICE&quot;: PRICE_COL,<br />}<br /><br />GT( # pandas dataframe serves as inupt to GT onject<br />    pd.concat( # aggregating data using pandas<br />        [<br />            df_NY.groupby(by=[&quot;LOCALITY&quot;])[<br />                [&quot;BEDS&quot;, &quot;BATH&quot;, &quot;PROPERTYSQFT&quot;, &quot;PRICE&quot;]<br />            ].median(),<br />            df_NY.groupby(by=[&quot;LOCALITY&quot;])[[&quot;ADDRESS&quot;]].count(),<br />        ],<br />        axis=1,<br />    )<br />    .reset_index()<br />    .rename(columns=rename_dict)<br />).fmt_integer( # table display from chained methods<br />    columns=[NO_BEDS_COL, NO_BATHS_COL, FLOOR_AREA_COL, NO_LISTINGS_COL]<br />).fmt_currency(<br />    columns=PRICE_COL<br />).tab_spanner(<br />    label=md(&quot;*Median property properties*&quot;),<br />    columns=[NO_BEDS_COL, NO_BATHS_COL, FLOOR_AREA_COL, PRICE_COL],<br />).tab_source_note(<br />    source_note=f&quot;Source: {source_address}&quot;<br />).tab_header(<br />    title=html(&quot;&lt;strong&gt;Properties across New York&lt;/strong&gt;&quot;),<br />    subtitle=html(&quot;Average property properties by New York localities&quot;),<br />)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vAMobWeqWbdzh2IspHuaKQ.png"/></figure> <p>As the code snippet shows, the ability to chain methods together allows fine control of the components of the table, including formatting and naming the source.</p> <p>A similar exercise below on sales from a Walmart dataset allowed the sales across different stores to be summarised easily.</p> <pre>df_sales = pd.read_csv(WALMART_SALES_FILE)<br />df_sales[&quot;Date&quot;] = pd.to_datetime(df_sales[&quot;Date&quot;], format=&quot;%d-%m-%Y&quot;)<br /><br /># Only selecting some stores as a sample<br />df_sales = df_sales.loc[df_sales[&quot;Store&quot;] &lt; 6, :]<br /><br />df_sales.loc[:, &quot;Month&quot;] = df_sales[&quot;Date&quot;].dt.month<br /><br /># calculating percentage of sales per month<br />groupby = df_sales.groupby(by=[&quot;Month&quot;, &quot;Store&quot;])[&quot;Weekly_Sales&quot;].mean().unstack()<br />groupby_percentage_sales_per_month = groupby / groupby.sum(axis=0)<br />groupby_percentage_sales_per_month.reset_index(inplace=True)<br /><br />groupby_percentage_sales_per_month[&quot;Month&quot;] = groupby_percentage_sales_per_month[<br />    &quot;Month&quot;<br />].apply(lambda x: calendar.month_abbr[x])<br /><br />groupby_percentage_sales_per_month.columns = (<br />    groupby_percentage_sales_per_month.columns.astype(&quot;str&quot;)<br />)<br /><br />GT(<br />    groupby_percentage_sales_per_month,<br />    rowname_col=&quot;Month&quot;,<br />).data_color(<br />    # domain=[0.8, 0.10],<br />    palette=[&quot;rebeccapurple&quot;, &quot;white&quot;, &quot;orange&quot;],<br />    na_color=&quot;white&quot;,<br />).tab_header(<br />    title=&quot;Average percentage yearly sales by month for five Walmart stores&quot;,<br />    # subtitle=html(&quot;Average monthly values at latitude of 20&amp;deg;N.&quot;),<br />).tab_source_note(<br />    source_note=f&quot;Source: https://www.kaggle.com/datasets/mikhail1681/walmart-sales&quot;<br />).fmt_percent(<br />    [<br />        &quot;1&quot;,<br />        &quot;2&quot;,<br />        &quot;3&quot;,<br />        &quot;4&quot;,<br />        &quot;5&quot;,<br />    ],<br />    decimals=2,<br />)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*yRQ9M4wcVhIdKQIFqCCPNw.png"/></figure> <p>Again, with a few chained methods, we have a table that allows us to clearly see some trends across these five different shops. You can quickly see how (unsurprisingly) sales peak in December, with lower fractions of sales in January and July.</p> <p><strong>My take: </strong>This is a well-developed package that makes creating tables for presentation straightforward. Taking pandas dataframes as inputs to the GT class, along with the option of chaining of formatting methods, makes this a straightforward package to adapt. As a researcher, if this could export to LaTeX format, that would be amazing. Thankfully <a href="https://github.com/posit-dev/great-tables/issues/75">someone has already submitted an issue asking for this as a feature</a>, with an estimation of “done in early-to-mid 2024”, so this feature is eagerly awaited</p> <h3>Conclusion</h3> <p>Hopefully you found this article somewhat interesting and useful. I believe the breadth of the packages here strongly underline the strengths of the scientific python stack. The notebook the plots and tables in this article were created in can be found on my GitHub <a href="https://github.com/conorhamill36/viz_packages">here</a>. Please share any constructive criticism, including anything key in the packages I missed, and any other visualisation packages that expand upon pandas/scikit-learn/matplotlib well!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e0dc4ca54302" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/swlh/visualisation-libraries-in-python-for-expanding-your-data-storytelling-e0dc4ca54302">Visualisation libraries in python for expanding your data storytelling</a> was originally published in <a href="https://medium.com/swlh">The Startup</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">All in This Together? Trends in COVID-19 Fatality Across London Boroughs From Machine Learning</title><link href="https://conor-hamill.github.io/blog/2020/all-in-this-together-trends-in-covid-19-fatality-across-london-boroughs-from-machine-learning/" rel="alternate" type="text/html" title="All in This Together? Trends in COVID-19 Fatality Across London Boroughs From Machine Learning"/><published>2020-10-17T09:39:32+00:00</published><updated>2020-10-17T09:39:32+00:00</updated><id>https://conor-hamill.github.io/blog/2020/all-in-this-together-trends-in-covid-19-fatality-across-london-boroughs-from-machine-learning</id><content type="html" xml:base="https://conor-hamill.github.io/blog/2020/all-in-this-together-trends-in-covid-19-fatality-across-london-boroughs-from-machine-learning/"><![CDATA[<p><em>Keywords: COVID-19, London, city, data science, pandas, geopandas, scikit-learn, geospatial, maps, race, matplotlib, modeling, boroughs, numpy, selenium, python, machine learning, data visualization</em></p> <blockquote>Author’s note: this was a short project done several years ago and contains some non-rigorous modelling and visualisation missing key components. So while it remains an interesting data exploration project, I would discourage anyone from drawing definite conclusions using these methods.</blockquote> <p>From the end of 2019, the SARS-CoV-2 pandemic has flooded our newsfeeds and headlines with a vast array of numbers; we measure the local growth using the R number, compare the effectiveness of lockdown measures in different countries using cases per population number, and we see the deaths that the official COVID fatalities miss through the excess deaths. Another measure of the seriousness of the disease, and one which can highlight where adequate medical care is not being brought, is the ratio of the deaths due to COVID and the cases diagnosed in an area. This is the quantity I will be exploring in this project.</p> <p>We know that there are many health factors that affect the COVID-19 mortality rate, including weight, age, smoking and underlying conditions like diabetes and asthma. One of the more shocking statistics to emerge, however, has been the strong positive correlation between COVID prevalence and non-white persons. <a href="https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-race-ethnicity.html">An analysis from the CDC published in August</a> showed that, compared to white individuals, hispanic or Latino persons are 2.8 times as likely to catch the virus, American Indian or Alaska native persons are 5.3 times as likely to be hospitalised, and Black or African American persons have a fatality rate 2.1 times as high. These statistics are stark indicators, that despite what recent advertisements like to say, for many people, we’re most certainly not in this together.</p> <p>To examine this discrepancy in COVID-19 prevalence across different groups and attempt to somewhat visualise it, I decided to look at the cases and deaths of the disease across the boroughs of London. I knew the Office for National Statistics (ONS) would have more detailed versions of the information I was looking for to investigate the trends, and as a large, international city with many different groups in it, London would provide me with the range of data I was looking for to gain some understanding of what affects a person’s chances of surviving catching the coronavirus.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*MKncdJRLtbZ9sNGj"/><figcaption>Credit: Zoltan Gabor Photography</figcaption></figure> <h3>The Data</h3> <p>I pulled together a range of data sources to use in this project, which I will briefly describe below.</p> <p>To visualise the London boroughs, I used a GeoJSON file from the website of Stuart Grange, a postdoctoral researcher at Empa, Switzerland. This file can be found <a href="https://skgrange.github.io/www/data/london_boroughs.json">here</a>.</p> <p>The ONS regularly publishes information on the deaths attributed to the coronavirus online, which can be found <a href="https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/datasets/deathsinvolvingcovid19bylocalareaanddeprivation">here</a>. It also breaks down information on the positive case tests in to regions, which can be found <a href="https://coronavirus.data.gov.uk/cases">here</a>.</p> <p>The demographics of London have its own <a href="https://en.wikipedia.org/wiki/Demography_of_London">Wikipedia page</a>, and it was from this that I was able to scrape the racial makeup of each borough (originally from the 2011 UK census).</p> <p>While censuses in the UK take place every 10 years, <a href="https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/populationestimatesforukenglandandwalesscotlandandnorthernireland">the ONS continues to estimate the population changes in each of its administrative areas in between these</a>. This includes ages, sex, population densities and immigrations.</p> <h3>Cleaning the Data</h3> <p>Firstly, I imported the GeoJSON file, and simply plotting the dataframe shows the geographic data.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8863d4721e0c668431ee6b1aaba86fae/href">https://medium.com/media/8863d4721e0c668431ee6b1aaba86fae/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/614/1*6BYBarxvNsW7ducir_ZhPQ.png"/><figcaption>London geoJSON data</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ohx-3kilApkP7sWvoebpuQ.png"/><figcaption>London from above with Google Maps</figcaption></figure> <p>The geoJSON data seems to match what we expect, so we’ll move forward with importing the COVID deaths for each borough.</p> <p>The ONS reports a lot of information that’s heavily annotated, so a bit of cleaning was involved. This included removing the first few rows of the datasheet, dropping columns we weren’t interested in and renaming others, shortening the names of regions, and removing the non-COVID death columns.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5570427b32c4421aca8538b967a8dfbc/href">https://medium.com/media/5570427b32c4421aca8538b967a8dfbc/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/589/1*rh3SJAt3rcMDoWFKe_ChDA.png"/><figcaption>Deaths compiled by the ONS for London boroughs.</figcaption></figure> <p>A similar method for cleaning the COVID cases data is shown below. However, as the cases are given in week numbers, they must be converted to dates. For this a function was made, which was then implemented by making a dictionary in a list comprehension.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/12cd31ded63dfeef76839365c6396afa/href">https://medium.com/media/12cd31ded63dfeef76839365c6396afa/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/911/1*tsjdDIpDo1aadB4WgMweAQ.png"/><figcaption>New column of summed cases highlighted.</figcaption></figure> <p>The summed cases until the month of July have now been added to the dataframe, and appear as how we would expect.</p> <p>The race data can now be extracted from the Wikipedia page. This was done using the selenium and BeautifulSoup libraries.</p> <p>The table of race demographics is contained in a table of class ‘wikitable sortable’. In this table, the values are held in &lt;td&gt; tags, and the race headings and boroughs are held in &lt;th&gt; tags. Functions were defined to extract the values and boroughs from each of these tags. These were saved to arrays using list comprehensions, the data was reshaped and then formed into a dataframe.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ff70a1c377f36dd9870fb47c39737ed2/href">https://medium.com/media/ff70a1c377f36dd9870fb47c39737ed2/href</a></iframe> <p>The dataframe we’ve extracted matches what we would expect.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*gldOoQpgIhpBvJ1hUb21CA.png"/><figcaption>Races across the London boroughs, extracted from a Wikipedia article.</figcaption></figure> <p>All these data sources are easily joined together using pandas concat function, with the borough name set as the index. The ratio of deaths to cases can be added to the dataframe easily using pandas. Taking a look at this across London shows a significant spread.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f973227b26ecac507d9244ce5d8d4a2e/href">https://medium.com/media/f973227b26ecac507d9244ce5d8d4a2e/href</a></iframe> <figure><img alt="" src="https://cdn-images-1.medium.com/max/670/1*ilnw69Mhdo2mhvNwLE1oRg.png"/><figcaption>Deaths:cases ratio for each London borough</figcaption></figure> <p>The median ages for each borough are extracted from the population estimates done by the ONS. Using the areas provided in the geoJSON file, population densities for each borough can be found.</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/57f972e1c01590e99923ae9c07cbbfc4/href">https://medium.com/media/57f972e1c01590e99923ae9c07cbbfc4/href</a></iframe> <h3>Measuring the trends using linear regression</h3> <p>To model the data, scikit-learn’s ridge regression model was used, to reduce the chances of overfitting, which seems likely given the co-linearity of some of the variables I have chosen. A function to make a model and plot it against the known data was used.</p> <p>When a model is intended to make future predictions, and thus must have its accuracy measured, the data is split into training and testing sets. In this case, I am only interested in the trends of the data and thus shall simply be using the entire dataset (consisting of 33 boroughs, not exactly very large).</p> <iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4cabee1c6a32df2419018f463ae48aae/href">https://medium.com/media/4cabee1c6a32df2419018f463ae48aae/href</a></iframe> <p>Modelling using all the features we have incorporated into our dataframe provides us with coefficients for each of our variables, as well as a visual comparison against the known death:case ratio.</p> <pre>Features : [&#39;White&#39;, &#39;Black&#39;, &#39;Asian&#39;, &#39;Other&#39;, &#39;Median Age&#39;, &#39;Population Density&#39;]<br />Coefficients: [[-0.00021014  0.00180947  0.00084168  0.00486416  0.00339385 -0.00033938]]<br />RMSE: 0.024022110590386066<br />Coefficient of determination (r^2): 0.4202066115941455</pre> <p>There’s a few insights we can quickly gleam from these results. In terms of the model fit, a value of R^2=0.42 shows that, while the model is far from a perfect fit, it’s at least doing significantly better that a purely random model, and there is some correlation in the feature and target varibles. The coefficients are definitely interesting. While there is a negative correlation between the COVID death:cases ratio and the proportion of white people in a borough, the model suggests that the expected fatality of the virus increases with the proportion of every other race in that borough.</p> <p>In fact, if we take the magnitudes of the coefficients as indications of the dependence of these variables on the fatality of the disease, race seems to have roughly the same effect as the medium age. In fact, population density, with a coefficient an order of magnitude, places a surprisingly small role. The model is shown against the original data below, both as a map and as a scatter plot.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/823/1*4kQZmdF_eYygAOPYlRMq-Q.png"/><figcaption>Death:cases ratio for the London broughs (original data on left, predictions of linear regression model on right).</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/713/1*gW32bFpYCbR5NBLdmiPP3g.png"/><figcaption>Death:cases ratio for London boroughs to the end of July (original data in blue, predicted in red).</figcaption></figure> <h3>Conclusions and Moving Forward</h3> <p>In conclusion, this model seems to strongly suggest that in the boroughs of London, race played a significant role in the chances of COVID-fatality at the end of July. Having a socioeconomic statistic like this be on par with a physical health measure is a serious aspect of the diseases progression, which is due a full explanation.</p> <p>The range of COVID-related fatalities across London shows that the help available is not evenly and fairly distributed. The relation of race to COVID-fatality could be due to biological reasons, or as has been seen with previous natural disasters and health crises, its relationship with poorer economic status, more cramped households, and poorer access to facilities. A <a href="https://www.nature.com/articles/s41586-020-2521-4">Nature paper examining factors associated with COVID-19-related death</a> concluded that “South Asian and Black people had a substantially higher risk of COVID-19-related death than white people, and this was only partly attributable to comorbidities, deprivation or other factors”, strongly implying this socioeconomic factors are playing a significant role.</p> <p>Obviously this project is meant to be far from the end-all and be-all of examining the factors affecting COVID-19 fatality in London, and there are several prominent limitations. Firstly, it has been well documented that in the first half of 2020, the testing facilities of the UK were completely overwhelmed, so we know that number of cases documented was very far from the true value. In addition, the racial demographics were taken from the 2011 census and may have shifted in the past few years. Simply taking the median age of a population may not give a true representation of the ages of the population; a care home in a relatively youthful borough would not be represented by this statistic.</p> <p>In terms of producing a model that could make better predictions, further work is planned in to adding new features and performing some feature selection. Another interesting insight into the progression of the pandemic would involve using more recent data to see if this correlation with race has continued as measures to mitigate the virus and the use of new drugs like dexamethasone have begun to be used.</p> <p>That’s the end of my project. I hope you see how a small amount of web scraping, data manipulation and machine learning can give us a bit of an insight into the forces that are changing our modern world. Thank you for your time and any feedback is appreciated!</p> <p>Feel free to connect with me on <a href="https://www.linkedin.com/in/conor-hamill-53961419a/">LinkedIn</a>, and the code used in this project can be found at <a href="https://github.com/conorhamill36/London_COVID">https://github.com/conorhamill36/London_COVID</a>.</p> <p>This project was written on a jupyter notebook hosted on an AWS EC2 instance. This can be set up using the instructions found in <a href="https://chrisalbon.com/aws/basics/run_project_jupyter_on_amazon_ec2/">this guide by Chris Albon</a>. The use of geoJSON data with GeoPandas was inspired by <a href="https://towardsdatascience.com/predicting-airbnb-prices-with-machine-learning-and-location-data-5c1e033d0a5a">this article</a> from <a href="https://towardsdatascience.com/@gracecarrilloc?source=post_page-----5c1e033d0a5a--------------------------------">Graciela Carrillo</a>, modelling Airbnb prices in Edinburgh.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e481c99d5cf0" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/swlh/all-in-this-together-trends-in-covid-19-fatality-across-london-boroughs-from-machine-learning-e481c99d5cf0">All in This Together? Trends in COVID-19 Fatality Across London Boroughs From Machine Learning</a> was originally published in <a href="https://medium.com/swlh">The Startup</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry></feed>